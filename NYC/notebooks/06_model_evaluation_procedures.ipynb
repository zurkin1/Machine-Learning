{
 "metadata": {
  "name": "",
  "signature": "sha256:ccea3b12dbd9d0bc247abfb693b91de332ed10a26258dd0dce7138bca03b0fc4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Model Evaluation Procedures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Motivating question:** Last time, we created classification models on the iris data to predict species using the iris measurements. We used KNN, with K=1 and K=5. But which model is \"better\"? And more importantly, what's the \"best\" value of K?\n",
      "\n",
      "To choose between models, we need two things:\n",
      "\n",
      "1. **Evaluation procedure:** the process you use to evaluate a model\n",
      "2. **Evaluation metric:** the numeric calculation you use to compare models\n",
      "\n",
      "In supervised learning, we define the \"best\" model as one that generalizes to \"out-of-sample\" data. In other words, we want a model that accurately predicts the future, not the past. Our evaluation procedure should support that goal. We will focus on evaluation procedures in today's class.\n",
      "\n",
      "Choosing an evaluation metric is a more subjective decision. The appropriate evaluation metric can depend upon the goal of your problem. We will focus on evaluation metrics in a future class."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Evaluation Procedure #1: Train and Test on Entire Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by training on the entire iris dataset, and then testing our model by seeing how well it performs on that same data. That will be our evaluation procedure.\n",
      "\n",
      "We'll use **classification accuracy** as our evalutaion metric, which is the percentage of correct predictions. This is called a \"reward function\" because it's something we want to maximize.\n",
      "\n",
      "The opposite of classification accuracy is **classification error**, which is the percentage of incorrect predictions. This is called a \"loss function\" because it's something we want to minimize."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the iris data\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "\n",
      "# create X (features) and y (response)\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# KNN with K=5\n",
      "from sklearn.neighbors import KNeighborsClassifier  # import class\n",
      "knn = KNeighborsClassifier(n_neighbors=5)           # instantiate the estimator\n",
      "knn.fit(X, y)                                       # train on entire dataset\n",
      "y_pred = knn.predict(X)                             # make predictions (or \"test\") on entire dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's compute the accuracy. This is known as **training accuracy** because we are testing on the same data we used to train the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# method 1: use metrics module\n",
      "from sklearn import metrics\n",
      "print metrics.accuracy_score(y, y_pred)\n",
      "\n",
      "# method 2: use NumPy\n",
      "import numpy as np\n",
      "print np.mean(y == y_pred)\n",
      "\n",
      "# method 3: use built-in \"score\" method (allows you to skip the knn.predict step)\n",
      "print knn.score(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.966666666667\n",
        "0.966666666667\n",
        "0.966666666667\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I recommend **method 1** because this pattern can be reused for different evaluation metrics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now try KNN with K=1\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X, y)\n",
      "y_pred = knn.predict(X)\n",
      "print metrics.accuracy_score(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's amazing! So, K=1 is the best value for K, right?\n",
      "\n",
      "Not so fast...\n",
      "\n",
      "Training accuracy rewards overly complex models that do not generalize to out-of-sample data. Setting K=1 caused KNN to effectively memorize the training data. Thus, it will do very well when tested using the in-sample data, but may do very poorly on out-of-sample data. **As such, training accuracy is not a good estimate of out-of-sample accuracy,** and out-of-sample accuracy is what matters.\n",
      "\n",
      "Creating an unnecessarily complex model is known as **overfitting**, because it is learning the \"noise\" rather than the \"signal\".\n",
      "\n",
      "From Quora: [What is an intuitive explanation of overfitting?](http://www.quora.com/What-is-an-intuitive-explanation-of-overfitting/answer/Jessica-Su)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Overfitting\n",
      "\n",
      "![Overfitting](images/overfitting.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Underfitting vs. Overfitting\n",
      "\n",
      "![Underfitting vs. Overfitting](images/underfitting_overfitting.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Evaluation Procedure #2: Train/Test Split"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now try an alternative model evaluation procedure known as **train/test split**. (This is also known as the **test set approach** or the **validation set approach**.) Here's our plan:\n",
      "\n",
      "1. Split the dataset into two pieces: a **training set** and a **testing set**.\n",
      "2. Train the model on the training set.\n",
      "3. Test the model on the testing set, and evaluate how well we did.\n",
      "4. Repeat steps 2 and 3 using different **tuning parameters** (the value of K, in this case).\n",
      "\n",
      "Why do we believe this will allow us to build a model that generalizes? Because we are evaluating the model on **data that was not used to train the model**.\n",
      "\n",
      "Before we use this procedure, we first have to understand how the `train_test_split` function works."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Understanding the `train_test_split` function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create X: 5 observations and 2 features\n",
      "X = np.arange(1, 11).reshape((5, 2))\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1  2]\n",
        " [ 3  4]\n",
        " [ 5  6]\n",
        " [ 7  8]\n",
        " [ 9 10]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create y: response vector of size 5\n",
      "y = X.prod(axis=1)\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 2 12 30 56 90]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# randomly split the rows of X into two pieces\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_split = train_test_split(X)   # returns a list with two elements\n",
      "print X_split[0]\n",
      "print X_split[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1  2]\n",
        " [ 9 10]\n",
        " [ 3  4]]\n",
        "[[5 6]\n",
        " [7 8]]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use random_state parameter to always get the same random split\n",
      "X_split = train_test_split(X, random_state=2)\n",
      "print X_split[0]\n",
      "print X_split[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[3 4]\n",
        " [7 8]\n",
        " [1 2]]\n",
        "[[ 5  6]\n",
        " [ 9 10]]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \"unpack\" the list into two separate objects\n",
      "X_train, X_test = train_test_split(X, random_state=2)\n",
      "print X_train\n",
      "print X_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[3 4]\n",
        " [7 8]\n",
        " [1 2]]\n",
        "[[ 5  6]\n",
        " [ 9 10]]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# randomly split the rows of X and y into two pieces each\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# X has been split into two pieces\n",
      "print X_train\n",
      "print X_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[3 4]\n",
        " [7 8]\n",
        " [1 2]]\n",
        "[[ 5  6]\n",
        " [ 9 10]]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y has been split into two pieces with the same ordering\n",
      "print y_train\n",
      "print y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[12 56  2]\n",
        "[30 90]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![train_test_split](images/train_test_split.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Using the Train/Test Split Procedure on iris"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 0: load the iris data again\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 1: split data into a training set and a testing set\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
      "print X_train.shape\n",
      "print X_test.shape\n",
      "print y_train.shape\n",
      "print y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(112L, 4L)\n",
        "(38L, 4L)\n",
        "(112L,)\n",
        "(38L,)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 2: train the model on the training set (using K=1)\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_neighbors=1, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 3: test the model on the testing set, and check the accuracy\n",
      "y_pred = knn.predict(X_test)\n",
      "print metrics.accuracy_score(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.947368421053\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Step 4: repeat this process for K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(X_train, y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "print metrics.accuracy_score(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.973684210526\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K=1 had a higher **training accuracy** than K=5, but K=5 had a higher **testing accuracy** than K=1. Testing accuracy is a much better estimator of out-of-sample accuracy, and thus we would prefer to use K=5 in this situation.\n",
      "\n",
      "Training error tends to decrease as model complexity increases, whereas testing error tends to hit a \"sweet spot\" somewhere between minimal and maximal complexity."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Training Error](images/training_error.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's pretend we tried **many values of K**, and K=5 turned out to have the highest testing accuracy. (In other words, K=5 was the \"sweet spot\".) What would we conclude?\n",
      "\n",
      "- We would declare 5 to be the best value for K when using KNN on the iris dataset.\n",
      "- We would estimate that when given the measurements of an unknown iris, we would be able to correctly predict its species 97% of the time."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Making Predictions on Out-of-Sample Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are now given the measurements of a (truly) unknown iris, and are asked to predict its species. How do we do it?\n",
      "\n",
      "1. We re-train our model on the **entire dataset**, using the **best value for K** found in the previous step.\n",
      "2. We make our prediction, and are 97% sure that we will make the correct prediction.\n",
      "\n",
      "It is important to re-train your model on **all** of the data, otherwise you will be ignoring valuable training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train the model on X (not X_train) using K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make a prediction for an unknown iris\n",
      "out_of_sample = [5, 4, 3, 2]\n",
      "knn.predict(out_of_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What's Wrong with Train/Test Split for Model Evaluation?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What would happen if the `train_test_split` function had split the data differently? Would we get the same exact results as before?\n",
      "\n",
      "Let's check!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use a different random_state than last time\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(X_train, y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "print metrics.accuracy_score(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It turns out that test set accuracy is a **high variance estimate** of out-of-sample accuracy. In a future class, we will learn an alternative model evaluation procedure called **K-fold cross-validation** that will provide a more accurate estimate of out-of-sample accuracy, and thus will be a better procedure for choosing the \"best\" model.\n",
      "\n",
      "However, train/test split is still an incredibly useful evaluation procedure because of its **flexibility and speed**, and thus we will use it throughout the course."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}