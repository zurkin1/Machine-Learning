{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Refresh with linear regression using scikit-learn\n",
    "2. Attempt to use linear regression for classification\n",
    "3. See why logistic regression is a better alternative for classification\n",
    "4. Explain the form of logistic regression\n",
    "5. Explain how to interpret logistic regression coefficients\n",
    "6. Demonstrate how logistic regression works with categorical features\n",
    "7. Compare logistic regression with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predicting a Continuous Response using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# glass identification dataset\n",
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass.sort_values(by='al', inplace=True)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Pretend that we want to predict **ri**, and our only feature is **al**. How could we do it using machine learning?\n",
    "\n",
    "**Answer:** We could frame it as a regression problem, and use a linear regression model with **al** as the only feature and **ri** as the response.\n",
    "\n",
    "**Question:** How would we **visualize** this model?\n",
    "\n",
    "**Answer:** Create a scatter plot with **al** on the x-axis and **ri** on the y-axis, and draw the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using seaborn, create a scatterplot using seaborn of the al values vs the ri values\n",
    "sns.lmplot(x='al', y='ri', data=glass, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How would we draw this plot without using Seaborn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the same scatter plot of al and ri using Pandas .plot() method\n",
    "glass.plot(kind='scatter', x='al', y='ri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# equivalent scatter plot using Matplotlib directly\n",
    "plt.scatter(glass.al, glass.ri)\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('ri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a linear regression model.  What are our 6 steps for creating a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.ri\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make predictions for all values of X and add them to the glass dataframe as a new column named \"ri_pred\"\n",
    "glass['ri_pred'] = linreg.predict(X)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot those predictions connected by a line\n",
    "plt.plot(glass.al, glass.ri_pred, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('Predicted ri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put the plots together\n",
    "plt.scatter(glass.al, glass.ri)\n",
    "plt.plot(glass.al, glass.ri_pred, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('ri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher: interpreting linear regression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression equation: $y = \\beta_0 + \\beta_1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for al=2 using the equation\n",
    "linreg.intercept_ + linreg.coef_ * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for al=2 using the predict method\n",
    "linreg.predict(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine coefficient for al\n",
    "zip(feature_cols, linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** A 1 unit increase in 'al' is associated with a 0.0025 unit decrease in 'ri'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# increasing al by 1 (so that al=3) decreases ri by 0.0025\n",
    "1.51699012 - 0.0024776063874696243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for al=3 using the predict method\n",
    "linreg.predict(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Predicting a Categorical Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glass.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine glass_type\n",
    "glass.glass_type.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our task, so that we're predicting **household** using **al**. Let's visualize the relationship to figure out how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(glass.al, glass.household)\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a **regression line**, like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a linear regression model and store the predictions\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "linreg.fit(X, y)\n",
    "glass['household_pred'] = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scatter plot that includes the regression line\n",
    "plt.scatter(glass.al, glass.household)\n",
    "plt.plot(glass.al, glass.household_pred, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **al=3**, what class do we predict for household? **1**\n",
    "\n",
    "If **al=1.5**, what class do we predict for household? **0**\n",
    "\n",
    "We predict the 0 class for **lower** values of al, and the 1 class for **higher** values of al. What's our cutoff value? Around **al=2**, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n",
    "\n",
    "Therefore, we'll say that if **household_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# understanding np.where\n",
    "import numpy as np\n",
    "nums = np.array([5, 15, 8])\n",
    "\n",
    "# np.where returns the first value if the condition is True, and the second value if the condition is False\n",
    "np.where(nums > 10, 'big', 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform household_pred to 1 or 0\n",
    "glass['household_pred_class'] = np.where(glass.household_pred >= 0.5, 1, 0)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.scatter(glass.al, glass.household)\n",
    "plt.plot(glass.al, glass.household_pred_class, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Logistic Regression Instead\n",
    "\n",
    "Logistic regression can do what we just did and give us the probability of the prediction being correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)\n",
    "glass['household_pred_class'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "plt.scatter(glass.al, glass.household)\n",
    "plt.plot(glass.al, glass.household_pred_class, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With logistic regression we can easily get the **predicted probabilities** instead of just the **class predictions**, to understand how confident we are in a given prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilites of class 1\n",
    "glass['household_pred_prob'] = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities\n",
    "plt.scatter(glass.al, glass.household)\n",
    "plt.plot(glass.al, glass.household_pred_prob, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "print logreg.predict_proba(1)\n",
    "print logreg.predict_proba(2)\n",
    "print logreg.predict_proba(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column indicates the predicted probability of **class 0**, and the second column indicates the predicted probability of **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression:** continuous response is modeled as a linear combination of the features:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "**Logistic regression:** a binary response (true/false, 1/0, win/loss...) is modeled with a log function to produce a prediction and the probability the prediction is correct.\n",
    "\n",
    "\n",
    "$$Probability = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **logistic function** has some nice properties:\n",
    "- Takes on an \"s\" shape\n",
    "- Output is bounded by 0 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities again\n",
    "plt.scatter(glass.al, glass.household)\n",
    "plt.plot(glass.al, glass.household_pred_prob, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=2 using the predict_proba method\n",
    "logreg.predict_proba(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the coefficient for al\n",
    "zip(feature_cols, logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'household'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bottom line:** Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logistic regression beta values](images/logistic_betas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the $\\beta_0$ value shifts the curve **horizontally**, whereas changing the $\\beta_1$ value changes the **slope** of the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Using Logistic Regression with Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can still be used with **categorical features**. Let's see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a categorical feature\n",
    "glass['high_al'] = np.where(glass.al > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Seaborn to draw the logistic curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# original (continuous) feature\n",
    "sns.lmplot(x='al', y='household', data=glass, ci=None, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categorical feature\n",
    "sns.lmplot(x='high_al', y='household', data=glass, ci=None, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categorical feature, with jitter added\n",
    "sns.lmplot(x='high_al', y='household', data=glass, ci=None, logistic=True, x_jitter=0.05, y_jitter=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a logistic regression model\n",
    "feature_cols = ['high_al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg.predict(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print logreg.predict_proba(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg.predict(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print logreg.predict_proba(.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparing Logistic Regression with Other Models\n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Highly interpretable (if you remember how)\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "- Outputs well-calibrated predicted probabilities\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log-odds of the response\n",
    "- Works only on datasets with binary responses unless extended by breaking larger numbers of responses into multiple binary problems or Multinomial logistic regression is used\n",
    "- Performance is (generally) not competitive with the best supervised learning methods\n",
    "- Can't automatically learn feature interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
