{
 "metadata": {
  "name": "",
  "signature": "sha256:b7e3a62e1216c53fa3e7d0fa56c5373dfe7f58c3817a1468b0abbc52dfe7b6a7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Applying Naive Bayes classification to spam email"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's pretend we have an email with three words: \"Send money now.\" We want to classify that email as ham or spam.\n",
      "\n",
      "We'll use Naive Bayes classification:\n",
      "\n",
      "$$P(spam | \\text{send money now}) = \\frac {P(\\text{send money now} | spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
      "\n",
      "By assuming that the features (the words) are conditionally independent, we can simplify the likelihood function:\n",
      "\n",
      "$$P(spam | \\text{send money now}) \\approx \\frac {P(\\text{send} | spam) \\times P(\\text{money} | spam) \\times P(\\text{now} | spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
      "\n",
      "We could calculate all of the values in the numerator by examining a corpus of spam:\n",
      "\n",
      "$$P(spam | \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
      "\n",
      "We could repeat this process to calculate the probability that the email is ham:\n",
      "\n",
      "$$P(ham | \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
      "\n",
      "All we care about is whether spam or ham has the higher probability, and so we predict that the email is spam.\n",
      "\n",
      "What have we learned from this exercise?\n",
      "\n",
      "- The \"naive\" assumption of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
      "- The normalization constant (the denominator) can be ignored since it's the same for all classes.\n",
      "- The prior probability is basically irrelevant once you have a lot of features.\n",
      "- The Naive Bayes classifier can handle a lot of irrelevant features."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}