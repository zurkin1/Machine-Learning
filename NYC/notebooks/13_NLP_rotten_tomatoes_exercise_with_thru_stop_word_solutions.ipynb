{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Sentiment Analysis on Rotten Tomatoes quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from rt_critics.csv in the data folder of our DAT2 repo\n",
    "# at 'https://raw.githubusercontent.com/JamesByers/GA-SEA-DAT2/master/data/rt_critics.csv'\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/JamesByers/GA-SEA-DAT2/master/data/rt_critics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fresh is the column with ratings.  Count the number of each value in column fresh\n",
    "df.fresh.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vectorize the quotes and store it on a variable names Xcv\n",
    "vectorizer = CountVectorizer()\n",
    "Xcv = vectorizer.fit_transform(df['quote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the shape of dataframe Xcv\n",
    "Xcv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! We have more features than samples. This would ensure overfitting. Let's trim that number down to the top 5000, ranked by the term frequency across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an vectorizer object as a variable named vectorizer that includes just the top 5000\n",
    "# Hint: check the documentation for CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Create a new vectorized feature matix named Xcv with the new vectorizer\n",
    "Xcv = vectorizer.fit_transform(df['quote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the response vector y where the value is 1 if \"fresh\" and 0 if any other value than fresh\n",
    "y = np.where(df.fresh == 'fresh',1,0)\n",
    "# or\n",
    "# y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine the null accuracy\n",
    "max(y.mean(), 1 - y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "xtrain_cv, xtest_cv, ytrain_cv, ytest_cv = train_test_split(Xcv, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate performance of models using test train split\n",
    "log_reg = LogisticRegression().fit(xtrain_cv, ytrain_cv)\n",
    "print \"Accuracy: %0.2f%%\" % (100 * log_reg.score(xtest_cv, ytest_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the logistic Regression regularization parameter \"C\" to improve performance.\n",
    "# Evaluate performance of models using test train split\n",
    "log_reg = LogisticRegression(C=1.5).fit(xtrain_cv, ytrain_cv)\n",
    "print \"Accuracy: %0.2f%%\" % (100 * log_reg.score(xtest_cv, ytest_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bonus if you have extra time: Create a for loop to find the C value\n",
    "# that produces the most accurate model \n",
    "step = np.arange(0.5,10,0.5)\n",
    "for c in step:\n",
    "    log_reg = LogisticRegression(C=c).fit(xtrain_cv, ytrain_cv)\n",
    "    print \"C= \", c, \"  Accuracy: \", round(100 * log_reg.score(xtest_cv, ytest_cv),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "The performance isn't bad, but it's not great. Let's see if we can improve things by [using stop words](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify your vectorizer to also remove stop words (still allow only 5000 features)\n",
    "\n",
    "# create a new vectorizer object that only allows 5000 features\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new X called Xcvs\n",
    "Xcvs = vectorizer.fit_transform(df['quote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the converted data (Xcvs) into training and test sets\n",
    "xtraincvs, xtestcvs, ytraincvs, ytestcvs = train_test_split(Xcvs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate performance of models using the test data\n",
    "# Tune the regularization parameter, C, to improve performance.\n",
    "\n",
    "log_reg = LogisticRegression(C=1.0).fit(xtraincvs, ytraincvs)\n",
    "print \"Accuracy: %0.2f%%\" % (100 * log_reg.score(xtestcvs, ytestcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the regularization parameter, C, to improve performance.\n",
    "log_reg = LogisticRegression(C=1.5).fit(xtraincvs, ytraincvs)\n",
    "print \"Accuracy: %0.2f%%\" % (100 * log_reg.score(xtestcvs, ytestcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Alternate tuning of C using for loop\n",
    "step = np.arange(0.5,10,0.5)\n",
    "for c in step:\n",
    "    log_reg = LogisticRegression(C=c).fit(xtraincvs, ytraincvs)\n",
    "    print \"C= \", c, \"  Accuracy: \", round(100 * log_reg.score(xtestcvs, ytestcvs),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "If that didn't work, how about using tf-idf weighting?\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# edit this cell to create a TfidfVectorizer instead of a simple CountVectorizer\n",
    "# or start with your own model with CountVectorizer from the cells above\n",
    "\n",
    "# create vectorizer object\n",
    "# vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Create Xti and y\n",
    "# Xti = vectorizer.fit_transform(df['quote'])\n",
    "# Y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
    "\n",
    "# split the converted data into training and test sets\n",
    "# xtrainti, xtestti, ytrainti, ytestti = train_test_split(Xti, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate performance of the new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the regularization parameter, C, to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bonus: if you have time find the best value of C using a for loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf and stop words\n",
    "\n",
    "Do both together help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# edit this cell to create a TfidfVectorizer that uses stop words\n",
    "\n",
    "# create vectorizer object\n",
    "#vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# convert our documents and their labels into numpy arrays\n",
    "#Xtis = vectorizer.fit_transform(df['quote'])\n",
    "#y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
    "\n",
    "# split the converted data into training and test sets\n",
    "#xtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate performance of models\n",
    "# Tune the regularization parameter, C, to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Bonus: if you have time find the best value of C using a for loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Are you satisfied with these results? Why might you be less than satisfied? How can you explain the observed behavior? What are the next steps you would need to do to improve this classifier? If you have time remaining, try a few strategies out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continue playing here\n",
    "\n",
    "# Use pipeline to evaluate accuracy with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Next Steps\n",
    "\n",
    "The hardest part of creating a sentiment model is finding good training data. Googling 'sentiment analysis training data' or 'sentiment analysis test data' turns up a few freely available sources. Most of them are hosted by universities.\n",
    "\n",
    "But notice, determining the judgment of a movie review isn't the same task as determining the emotional content of a tweet. And yet, it kind of is. The computer doesn't know anything about nature of the text. All it knows is that there are documents with one label (fresh/happy) and documents with another label (rotten/sad) and it needs to fit a model to discriminate between the two. This can be extended to more classes (look into the 20 newsgroups dataset in sci-kit learn) and to proprietary corpora.\n",
    "\n",
    "One application you might use at work is classifying support emails from users. The classes may be 'ranting', 'mischarge', 'lost order', 'gushing'. Or whatever is common. Even if the classifier isn't perfect, it could help streamline the process of getting the right emails to the right support personnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<style>\n",
    ".text_cell_render {\n",
    "  background-color: silver\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
