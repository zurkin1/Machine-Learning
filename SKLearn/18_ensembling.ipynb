{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we learning about ensembling?\n",
    "\n",
    "- Very popular method for improving the predictive performance of machine learning models\n",
    "- Provides a foundation for understanding more sophisticated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson objectives\n",
    "\n",
    "Students will be able to:\n",
    "\n",
    "- Define ensembling and its requirements\n",
    "- Identify the two basic methods of ensembling\n",
    "- Decide whether manual ensembling is a useful approach for a given problem\n",
    "- Explain bagging and how it can be applied to decision trees\n",
    "- Explain how out-of-bag error and feature importances are calculated from bagged trees\n",
    "- Explain the difference between bagged trees and Random Forests\n",
    "- Build and tune a Random Forest model in scikit-learn\n",
    "- Decide whether a decision tree or a Random Forest is a better model for a given problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction\n",
    "\n",
    "Let's pretend that instead of building a single model to solve a binary classification problem, you created **five independent models**, and each model was correct about 70% of the time. If you combined these models into an \"ensemble\" and used their majority vote as a prediction, how often would the ensemble be correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "[1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0]\n",
      "[1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1]\n",
      "[1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0]\n",
      "[0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "# generate 1000 random numbers (between 0 and 1) for probability prediction for each model, representing 1000 predictions\n",
    "mod1 = np.random.rand(1000)\n",
    "mod2 = np.random.rand(1000)\n",
    "mod3 = np.random.rand(1000)\n",
    "mod4 = np.random.rand(1000)\n",
    "mod5 = np.random.rand(1000)\n",
    "\n",
    "# each model independently predicts 1 (the \"correct response\") if random number was at least 0.3 (that is the \"threshold\" for our\n",
    "# which achieves 70% correct, i.e. we assume that only on these cases we predicted correctly. On other samples we were wrong.)\n",
    "preds1 = np.where(mod1 > 0.3, 1, 0)\n",
    "preds2 = np.where(mod2 > 0.3, 1, 0)\n",
    "preds3 = np.where(mod3 > 0.3, 1, 0)\n",
    "preds4 = np.where(mod4 > 0.3, 1, 0)\n",
    "preds5 = np.where(mod5 > 0.3, 1, 0)\n",
    "\n",
    "# print the first 20 predictions from each model\n",
    "print(preds1[:20])\n",
    "print(preds2[:20])\n",
    "print(preds3[:20])\n",
    "print(preds4[:20])\n",
    "print(preds5[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average the predictions and then round to 0 or 1 - majority voting\n",
    "ensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 + preds5)/5.0).astype(int)\n",
    "\n",
    "# print the ensemble's first 20 predictions\n",
    "print (ensemble_preds[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713\n",
      "0.665\n",
      "0.717\n",
      "0.712\n",
      "0.687\n"
     ]
    }
   ],
   "source": [
    "# how accurate was each individual model?\n",
    "print (preds1.mean())\n",
    "print (preds2.mean())\n",
    "print (preds3.mean())\n",
    "print (preds4.mean())\n",
    "print (preds5.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841\n"
     ]
    }
   ],
   "source": [
    "# how accurate was the ensemble?\n",
    "print (ensemble_preds.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As you add more models to the voting process, the probability of error decreases, which is known as [Condorcet's Jury Theorem](http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ensembling?\n",
    "\n",
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model.\n",
    "\n",
    "- **Regression:** take the average of the predictions\n",
    "- **Classification:** take a vote and use the most common prediction, or take the average of the predicted probabilities\n",
    "\n",
    "For ensembling to work well, the models must have the following characteristics:\n",
    "\n",
    "- **Accurate:** they outperform the null model\n",
    "- **Independent:** their predictions are generated using different processes\n",
    "\n",
    "**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when averaging the models.\n",
    "\n",
    "What makes a good manual ensemble?\n",
    "- Different types of models\n",
    "- Different combinations of features\n",
    "- Different tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine learning flowchart](images/crowdflower_ensembling.jpg)\n",
    "\n",
    "*Machine learning flowchart created by the [winner](https://github.com/ChenglongChen/Kaggle_CrowdFlower) of Kaggle's [CrowdFlower competition](https://www.kaggle.com/c/crowdflower-search-relevance)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Bagging\n",
    "\n",
    "The primary weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially due to **high variance**, meaning that different splits in the training data can lead to very different trees.\n",
    "\n",
    "**Bagging** is a general purpose procedure for reducing the variance of a machine learning method, but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n",
    "\n",
    "What is a **bootstrap sample**? A random sample with replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n"
     ]
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# create an array of 1 through 20\n",
    "nums = np.arange(1, 21)\n",
    "print nums\n",
    "\n",
    "# sample that array 20 times with replacement\n",
    "print np.random.choice(a=nums, size=20, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does bagging work (for decision trees)?**\n",
    "\n",
    "1. Grow B trees using B bootstrap samples from the training data.\n",
    "2. Train each tree on its bootstrap sample and make predictions.\n",
    "3. Combine the predictions:\n",
    "    - Average the predictions for **regression trees**\n",
    "    - Take a vote for **classification trees**\n",
    "\n",
    "Notes:\n",
    "\n",
    "- **Each bootstrap sample** should be the same size as the original training set.\n",
    "- **B** should be a large enough value that the error seems to have \"stabilized\".\n",
    "- The trees are **grown deep** so that they have low bias/high variance.\n",
    "\n",
    "Bagging increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with train/test split (for estimating out-of-sample error) by splitting many times an averaging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged decision trees in scikit-learn (with B=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and prepare the vehicle training data\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_train.csv'\n",
    "train = pd.read_csv(url)\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})\n",
    "# read in and prepare the vehicle testing data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_test.csv'\n",
    "test = pd.read_csv(url)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "\n",
    "# define the training and testing sets\n",
    "X_train = train.iloc[:, 1:]\n",
    "y_train = train.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instruct BaggingRegressor to use DecisionTreeRegressor as the \"base estimator\"\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3344.2,   5395. ,  12902. ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and predict\n",
    "bagreg.fit(X_train, y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657.80003040437748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating out-of-sample error\n",
    "\n",
    "For bagged models, out-of-sample error can be estimated without using **train/test split** or **cross-validation**!\n",
    "\n",
    "On average, each bagged tree uses about **two-thirds** of the observations. For each tree, the **remaining observations** are called \"out-of-bag\" observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  2, 12,  2,  6,  1,  3, 10, 11,  9,  6,  1,  0,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# create ten bootstrap samples (will be used to select rows from the DataFrame)\n",
    "samples = [np.random.choice(a=14, size=14, replace=True) for _ in range(1, 11)]\n",
    "# show the first bootstrap sample\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 6, 9, 10, 11, 12, 13}\n",
      "{0, 1, 2, 3, 4, 7, 9, 13}\n",
      "{0, 2, 3, 4, 6, 7, 8, 9, 12, 13}\n",
      "{0, 1, 2, 3, 5, 6, 8, 10, 11, 12}\n",
      "{2, 3, 4, 6, 10, 11, 12, 13}\n",
      "{0, 1, 4, 5, 6, 7, 9, 10, 11}\n",
      "{0, 1, 2, 3, 4, 5, 8, 9, 12}\n",
      "{1, 2, 3, 5, 6, 7, 9, 11}\n",
      "{1, 3, 6, 7, 8, 9, 11, 12}\n",
      "{0, 1, 3, 4, 5, 6, 8, 10, 11, 13}\n"
     ]
    }
   ],
   "source": [
    "# show the \"in-bag\" observations for each sample\n",
    "for sample in samples:\n",
    "    print (set(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 7, 8]\n",
      "[5, 6, 8, 10, 11, 12]\n",
      "[1, 5, 10, 11]\n",
      "[4, 7, 9, 13]\n",
      "[0, 1, 5, 7, 8, 9]\n",
      "[2, 3, 8, 12, 13]\n",
      "[6, 7, 10, 11, 13]\n",
      "[0, 4, 8, 10, 12, 13]\n",
      "[0, 2, 4, 5, 10, 13]\n",
      "[2, 7, 9, 12]\n"
     ]
    }
   ],
   "source": [
    "# show the \"out-of-bag\" observations for each sample\n",
    "for sample in samples:\n",
    "    print (sorted(set(range(14)) - set(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate **\"out-of-bag error\":**\n",
    "\n",
    "1. For every observation in the training data, predict its response value using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a vote (for classification).\n",
    "2. Compare all predictions to the actual response values in order to compute the out-of-bag error.\n",
    "\n",
    "When B is sufficiently large, the **out-of-bag error** is an accurate estimate of **out-of-sample error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79869551339899825"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the out-of-bag R-squared score (not MSE, unfortunately!) for B=500\n",
    "bagreg.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating feature importance\n",
    "\n",
    "Bagging increases **predictive accuracy**, but decreases **model interpretability** because it's no longer possible to visualize the tree to understand the importance of each feature.\n",
    "\n",
    "However, we can still obtain an overall summary of **feature importance** from bagged models:\n",
    "\n",
    "- **Bagged regression trees:** calculate the total amount that **MSE** is decreased due to splits over a given feature, averaged over all trees\n",
    "- **Bagged classification trees:** calculate the total amount that **Gini index** is decreased due to splits over a given feature, averaged over all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Random Forests\n",
    "\n",
    "Random Forests is a **slight variation of bagged trees** that has even better performance:\n",
    "\n",
    "- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n",
    "- However, when building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**.\n",
    "    - A new random sample of features is chosen for **every single tree at every single split**.\n",
    "    - For **classification**, m is typically chosen to be the square root of p.\n",
    "    - For **regression**, m is typically chosen to be somewhere between p/3 and p.\n",
    "\n",
    "What's the point?\n",
    "\n",
    "- Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**.\n",
    "- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging).\n",
    "- By randomly leaving out candidate features from each split, **Random Forests \"decorrelates\" the trees**, such that the averaging process can reduce the variance of the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Building and tuning decision trees and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  sex  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris    1   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
       "2         1       3                             Heikkinen, Miss. Laina    0   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
       "4         0       3                           Allen, Mr. William Henry    1   \n",
       "5         0       3                                   Moran, Mr. James    1   \n",
       "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
       "\n",
       "         age  sibsp  parch            ticket     fare cabin embarked  \\\n",
       "0  22.000000      1      0         A/5 21171   7.2500   NaN        S   \n",
       "1  38.000000      1      0          PC 17599  71.2833   C85        C   \n",
       "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
       "3  35.000000      1      0            113803  53.1000  C123        S   \n",
       "4  35.000000      0      0            373450   8.0500   NaN        S   \n",
       "5  29.699118      0      0            330877   8.4583   NaN        Q   \n",
       "6  54.000000      0      0             17463  51.8625   E46        S   \n",
       "7   2.000000      3      1            349909  21.0750   NaN        S   \n",
       "8  27.000000      0      2            347742  11.1333   NaN        S   \n",
       "9  14.000000      1      0            237736  30.0708   NaN        C   \n",
       "\n",
       "   embarked_Q  embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  \n",
       "5           1           0  \n",
       "6           0           1  \n",
       "7           0           1  \n",
       "8           0           1  \n",
       "9           0           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the Titanic data\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/titanic.csv')\n",
    "\n",
    "# encode sex feature\n",
    "titanic['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in missing values for age\n",
    "titanic.age.fillna(titanic.age.mean(), inplace=True)\n",
    "\n",
    "# create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.embarked, prefix='embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "# note: axis=0 means rows, axis=1 means columns\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# create a list of feature columns\n",
    "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=600, n_jobs=1, oob_score=True, random_state=1,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import class, instantiate estimator, fit with all data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfclf = RandomForestClassifier(n_estimators=600, max_features=3, oob_score=True, random_state=1)\n",
    "rfclf.fit(titanic[feature_cols], titanic.survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most important tuning parameters for Random Forests:\n",
    "- n_estimators: more estimators (trees) increases performance but decreases speed\n",
    "- max_features: cross-validate to choose an ideal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pclass</td>\n",
       "      <td>0.159183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.374466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>0.424082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embarked_Q</td>\n",
       "      <td>0.012104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embarked_S</td>\n",
       "      <td>0.030164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      pclass    0.159183\n",
       "1         sex    0.374466\n",
       "2         age    0.424082\n",
       "3  embarked_Q    0.012104\n",
       "4  embarked_S    0.030164"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':rfclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80808080808080807"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the out-of-bag classification accuracy\n",
    "rfclf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1) Try changing the tuning parameters, n_estimators and max_features in order to find optimal values.\n",
    "\n",
    "2) Try chaning the titanic ensemble to bagging with the baseline model of your three favorite models. Use this ensemble to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NearestNeighbors' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-746dbd090cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ball_tree'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurvived\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moob_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_oob_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py\u001b[0m in \u001b[0;36m_set_oob_score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m                 \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NearestNeighbors' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# instruct BaggingRegressor to use DecisionTreeRegressor as the \"base estimator\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(NearestNeighbors(n_neighbors=2, algorithm='ball_tree'), n_estimators=200, bootstrap=True, oob_score=True, random_state=1)\n",
    "bag.fit(titanic[feature_cols], titanic.survived)\n",
    "bag.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints: Tuning n_estimators for RandomForestRegressor\n",
    "\n",
    "One important tuning parameter is **n_estimators**, which is the number of trees that should be grown. It should be a large enough value that the error seems to have \"stabilized\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of values to try for n_estimators\n",
    "estimator_range = range(10, 310, 10)\n",
    "\n",
    "# list to store the average RMSE for each value of n_estimators\n",
    "RMSE_scores = []\n",
    "\n",
    "# use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1860e710>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEQCAYAAACugzM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18154e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot n_estimators (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(estimator_range, RMSE_scores)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_features for RandomForestRegressor\n",
    "\n",
    "The other important tuning parameter is **max_features**, which is the number of features that should be considered at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of values to try for max_features\n",
    "feature_range = range(1, len(feature_cols)+1)\n",
    "\n",
    "# list to store the average RMSE for each value of max_features\n",
    "RMSE_scores = []\n",
    "\n",
    "# use 10-fold cross-validation with each value of max_features (WARNING: SLOW!)\n",
    "for feature in feature_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=150, max_features=feature, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=10, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x18d5a320>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEQCAYAAABbfbiFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWREnBzMzK+LkYGZmRf4/NxTupiNNzpgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1860e128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot max_features (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(feature_range, RMSE_scores)\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288.41877774269841, 8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the best RMSE and the corresponding max_features\n",
    "sorted(zip(RMSE_scores, feature_range))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Random Forests with decision trees\n",
    "\n",
    "**Advantages of Random Forests:**\n",
    "\n",
    "- Performance is competitive with the best supervised learning methods\n",
    "- Provides a more reliable estimate of feature importance\n",
    "- Allows you to estimate out-of-sample error without using train/test split or cross-validation\n",
    "\n",
    "**Disadvantages of Random Forests:**\n",
    "\n",
    "- Less interpretable\n",
    "- Slower to train\n",
    "- Slower to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine learning flowchart](images/driver_ensembling.png)\n",
    "\n",
    "*Machine learning flowchart created by the [second place finisher](http://blog.kaggle.com/2015/04/20/axa-winners-interview-learning-telematic-fingerprints-from-gps-data/) of Kaggle's [Driver Telematics competition](https://www.kaggle.com/c/axa-driver-telematics-analysis)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
